{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c7235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4049e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'paper.pdf'\n",
    "doc = convert_from_path(filePath)\n",
    "path, fileName = os.path.split(filePath)\n",
    "fileBaseName, fileExtension = os.path.splitext(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf46c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a511fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page # 0 - b'Retrieval-Augmented LLMs with Indonesian\\nClinical Trials Guidelines: A Comparative Study\\n\\nElla Raputri\\nComputer Science Department\\nSchool of Computing and Creative Arts\\nBina Nusantara University\\nJakarta, Indonesia 11480\\nella.raputri@binus.ac.id\\n\\nFeri Setiawan\\nComputer Science Department\\nSchool of Computing and Creative Arts\\nBina Nusantara University\\nJakarta, Indonesia 11480\\nferi.setiawan @ binus.edu\\n\\nAbstract\\xe2\\x80\\x94Artificial Intelligence (AI), especially Large\\nLanguage Model (LLM) powered chatbots, has emerged as a\\nsignificant tool in our daily lives. Many industries, including\\nthe healthcare industry nowadays use AI chatbot for their\\noperational tasks. One of such tasks is for diagnosing diseases\\nbased on the patient\\xe2\\x80\\x99s symptoms, so that the medical workers\\ncan focus on more complicated tasks. However, in practice,\\nLLM tends to generate incorrect information, so they need\\nto be provided with more specific medical-related knowledge.\\nTherefore, this study aims to enhance LLM diagnostic ability\\nby integrating Indonesian Clinical Trials Guidelines using\\nRAG (Retrieval-Augmented Generation) to give the LLMs\\nmore context on medical cases. After that, we compare the\\ndiagnostic ability of four different RAG-enhanced popular\\nLLMs (Claude, GPT, Deepseek, and Qwen). As a contrast\\nto the RAG LLM, we also compare their results with a\\nmodel that is not enhanced using RAG as a representative\\nof non-RAG LLM, which is Deepseek. Our research utilizes\\ncontext-related metrics with the ground truth from a general\\nmedical practitioner to evaluate all five models. The result\\nreveals that Claude has the highest performance with all\\nmetrics greater than 0.8 and GPT is the second best with all\\nmetrics greater than 0.65, followed by Deepseek with RAG,\\nQwen, and lastly, Deepseek without RAG, proving that RAG\\nhas a significant effect on the increase of diagnostic accuracy.\\n\\nIndex Terms\\xe2\\x80\\x94Large Language Model (LLM), diagnosing\\ndiseases, RAG (Retrieval-Augmented Generation), context-\\nrelated metrics\\n\\nI. INTRODUCTION\\n\\nIn recent years, Artificial Intelligence (AI) has become\\na topic that is intensively discussed in both research and\\nreal-world applications. Among all variations of AI, the\\nAI chatbot powered by Large Language Model (LLM) has\\nbeen a revolutionary one as it laid a foundation in creating\\nintelligent agents that can respond to user requests using\\nhuman-like languages [1]. This technological revolution\\ncan not be separated from the release of ChatGPT in 2022\\n[2], which deeply influenced the general public\\xe2\\x80\\x99s view\\nand curiosity in machine intelligence. Therefore, many\\n\\nAri Jaya Teguh\\nComputer Science Department\\nSchool of Computing and Creative Arts\\nBina Nusantara University\\nJakarta, Indonesia 11480\\nari.teguh @ binus.ac.id\\n\\nSaffanah Nur Hidayah\\nGeneral Practitioner\\nZamzam Clinic\\nSouth Jakarta, Indonesia 12550\\nsaffanah.nh @ gmail.com\\n\\nNunung Nurul Qomariyah\\nComputer Science Department\\nSchool of Computing and Creative Arts\\nBina Nusantara University\\nJakarta, Indonesia 11480\\nnunung.qomariyah @binus.edu\\n\\nindustries started to take an interest in AI and adopt it\\nin their operation processes.\\n\\nAdoption of AI technology in various industries has\\nbeen proven to increase productivity and creativity at\\nthe organizational level [3]. Among all industries, the\\nintegration of AI into the healthcare system has been a\\nserious topic as many people now rely on AI chatbots for\\ngathering information, including the diagnosis of diseases.\\nAccording to the study in [4], more than 20 percent of\\nhealth worker use ChatGPT for their health-related work.\\nReliance on AI chatbot, such as ChatGPT has become\\nmore prevalent these days as they can accelerate the\\ndiagnostic process, making it easier for patients and health\\nworkers to conclude a simple disease case [5]. Therefore,\\nhealth workers can focus on more complex tasks that need\\ntheir attention.\\n\\nHowever, as with other technologies, AI also has its lim-\\nitations. To date, many AI systems have been developed to\\nhelp the healthcare industry. However, adoption and eval-\\nuation are still limited [6]. Besides that, AI chatbots tend\\nto \\xe2\\x80\\x9challucinate\\xe2\\x80\\x9d or generate nonsensical information, so\\npractical deployment of them to diagnose symptoms has\\nalso been a problem [7]. Hence, to enhance the reliability\\nof the chatbot\\xe2\\x80\\x99s response in a medical context, we combine\\nLLMs with RAG (Retrieval-Augmented Generation) using\\ninformation from PPPK (Panduan Praktik Profesional Ke-\\ndokteran) or Indonesian Clinical Trials Guidelines. After\\nthat, we conduct evaluation to determine the performance\\nof each model in the diagnostic process.\\n\\nIn conclusion, our research aims to compare the diag-\\nnostic ability of popular LLMs, such as Claude, Deepseek,\\nQwen, and GPT. To enhance every models\\xe2\\x80\\x99 performance,\\nwe provide relevant medical context to them by using\\nRAG. Therefore, it is hoped that each model can provide\\na more accurate response, and we can benchmark them\\nto decide which one is the best in providing medical\\ndiagnoses.\\n'\n",
      "Page # 1 - b'II. RELATED WORKS\\nA. Large Language Models in Healthcare\\n\\nLarge Language Models (LLMs) are emerging in our\\ndaily lives, especially in the medical field. An LLM is\\nused to understand the relationship between words to get\\nthe context between them. The AI model can determine a\\ndiagnosis through learning from large datasets of medical\\ntext, thus confirming their potential in the medical field\\n[8]. The emerging LLMs influence is substantiated by the\\npublications of 175 journals [9].\\n\\nGoh [10] integrated LLMs into diagnostic workflows\\nin random clinical trials, resulting in a more accurate and\\nfaster time to diagnose a case. Another study did a slight\\ndifferent way, by comparing two AI (GPT-3.5 and GPT-4)\\nand comparing them on which model can generate a more\\naccurate and reliable diagnosis. The outcome displayed the\\npotential of LLMs in diagnostic support for clinics [11].\\n\\nRegardless of what perks LLMs given, it still faces\\nwith issues with unreliability and accuracy, especially\\nin high-risk scenario, where misdiagnosis can lead to\\nfatality. A survey consisting of 550 studies displays both\\nthe possibilities of LLMs and also their drawbacks [12].\\nAccording to Chiu [13], in order to overcome the issues of\\naccuracy and reliability of LLM, human oversight is still\\nrequired to ensure there are no misdiagnoses, especially\\nin a high-risk case.\\n\\nB. Retrieval-Augmented Generation in Medical Context\\n\\nRetrieval-Augmented Generation (RAG) is a method\\nthat combines external data resources like clinical guide-\\nlines, scientific literature databases, electronic health\\nrecords, etc. Gargari also stated that RAG has become\\nan important weapon to combat the challenges of stand-\\nalone LLM in healthcare [14]. By providing external data\\nresources that the models have not been programmed\\nwith, it also aids in preventing time-sensitive bias and\\nmislabeled information caused by human errors [15].\\n\\nA study by Miao [16] displayed the benefits, which\\nare improving the accuracy and the relevance of medical\\noutputs, on combining LLM and RAG in nephrology to\\ndiagnose a case. A similar study by Bora [17] also exhib-\\nited the improvement on the outcome by integrating RAG-\\nbased databases and LLMs with medical text, especially\\nwhere computing resources are scarce.\\n\\nThis has led to the generalizable benchmarking study\\nthat, in the field of medical retrieval-augmented genera-\\ntion, provides evaluation suites dedicated to the field that\\naccount for over 7,000 sets of clinical queries [18]. Sta-\\ntistically significant gains have been indicated in retrieval-\\naugmented generation following the reinforcement of the\\nlatter mechanism with RAG in the context of summarizing\\ntasks up to 6% of the purpose of clinical information\\nextraction as measured by clinical empirical studies [19].\\n\\nC. Evaluation Metrics for Medical AI Systems\\n\\nBLEURT, a learned metric based on BERT, was trained\\non millions of synthetic instances and followed up by\\nfine-tuning on human scores, in order to estimate coher-\\nent judgment even in the regime of small and out-of-\\ndistribution data [20]. The syntactic and semantic mis-\\nmatch combined with the surface lexical coverage makes\\n\\nit resilient to change of domain- an absolute necessity of\\nmedical AI evaluation where lexicon and phrasings are\\nhighly witness altering.\\n\\nIn that sense, BERTScore is superior to n-gram based\\nmetrics as the comparison is performed by the to-\\nkens of the lens of contextual embeddings, therefore,\\nto the rephrasing and long dependence of the context\\nof meaning-homogeneous sentences. These are the main\\ncharacteristics that make BERTScore particularly conve-\\nnient to use in situations where what counts most is the\\nprecision of meaning, as in the case of clinical use [21].\\n\\nIn the meantime, Semantic Answer Similarity (SAS)\\nintroduces a related concept, semantic assessment to\\nquestion-answering task context because this task employs\\na friendlier or harsher definition of the intimate contact\\nbetween a question and an answer. SAS has the capabil-\\nity of gauging outcomes that are comparable to human\\nbeings. Therefore, SAS may consider the same, although\\nit produces various lexical responses to it. This specialty\\nis useful to assist in rating the medical responses, which\\nis most of the time in a free-text form [22].\\n\\nIn more recent times, there is G-EVAL allowing deploy-\\nment of massive language models (GLM; GPT-4 or the\\nlike). Here, the output is assessed according to provided\\nclinical and semantic criteria by assessors independently,\\nand separate from use texts. While, on one hand, G-\\nEVAL was observed closer to them in terms of human\\njudgment for open-ended and fine-prone tasks empirically;\\nin contrast, prompting based on LLM-full texts could be\\nbiased in evaluation it may introduce [23].\\n\\nD. Medical AI Evaluation in Non-English Languages\\n\\nEven though the use of LLM in medical regions focuses\\nmore on the English language, other linguistic contexts are\\nbeginning to be recognized. In Indonesia, it is impossible\\nto practice without local language skills. Culturalization\\nof medical AI framework needs specific problems that\\npresent language peculiarities in the Indonesian context\\nand cultural factors, as well as the translation process to\\nsome names or terms in the medical area.\\n\\nWhen the research issues concerning medical AI assess-\\nment are considered through the perspective of Indonesia,\\na gap in the existing literature of a pronounced type is\\nobserved. The few existing literature cannot cover the\\nhuge field that LLM offers. So, it is critical to design the\\nmodel so that the AI can still operate like normal even if\\nthe model is offered a non-English medical text.\\n\\nIII. METHODOLOGY\\nA. Dataset Overview and Description\\n\\nThe current dataset consists of 45 medical cases that\\nhave been carefully selected, in order to reflect a wide\\nrange of diagnostic situations of a typical clinical practice.\\nIn every case description, there is the exact syndrome\\nof a patient in Indonesian. The dataset also includes\\nthe respective ground truth medical diagnoses verified\\nby competent licensed medical practitioners familiar with\\nIndonesian medical language and Indonesian Clinical Tri-\\nals Guidelines. Besides ground truth, the dataset also\\nconsists of the answer from each model. There are two\\ndifferent columns, which are the answer and the full\\n'\n",
      "Page # 2 - b'answer. The answer column contains the diseases that the\\nmodel predicts or diagnoses based on the medical cases.\\nMeanwhile, the full answer column is the full answer from\\nthe model, including the conventional greetings and other\\nunnecessary information. The columns of the dataset can\\nbe seen in Table I.\\n\\nB. Dataset Preprocessing\\n\\nFirst of all, we prepared a set of sample questions\\nthat represent various medical cases. Then, we gather\\nthe answers (diagnoses for the medical cases) from dif-\\nferent Large Language Models by prompting them. To\\nincrease response accuracy, we use the RAG (Retrieval\\nAugmented Generation) approach using the previously\\nstored Indonesian Clinical Trials Guidelines document\\ninside the AstraDB vector database. Therefore, the models\\ncan now use the document as their reference source when\\ngiving diagnoses.\\n\\nAfter the data gathering step, we obtained the full\\ndiagnostic answer from each model. From the full answer\\nof each model, we extract the predicted diseases. Then,\\nwe invited a medical practitioner to establish the ground\\ntruth for each sample medical case.\\n\\nThe doctor provides the diagnoses for the medical cases\\nbased on her experience, but she mainly diagnoses the\\nmedical cases using Claude\\xe2\\x80\\x99s diagnoses. In other words,\\nshe read all the diagnosis results from each model and\\nsaw that each of them is similar. Hence, she only read\\nClaude\\xe2\\x80\\x99s diagnoses and determined which one of the five\\ndiseases in the Claude output was the most likely based\\non the questions. The result that she provided is the main\\ndisease (the most potential to be the correct answer for\\nthe question) and the differential diagnosis (other possible\\ndiseases based on the symptoms in the question).\\n\\nC. Model and Techniques\\n\\n1) Large Language Models Selection: Four well-\\nknown LLMs were chosen to be tested under two con-\\nditions of the experiment, one being a standalone mode\\n(no external knowledge is supplemented to LLMs) and\\nanother being enhanced mode, provided by RAG. The\\nmodels that were selected are Claude 3.5 Haiku, QWEN\\n2.5 72B, GPT-40 mini, and Deepseek-V3.\\n\\n2) RAG Implementation: The RAG system is devel-\\noped on the basis of incorporating Indonesian medical\\nknowledge contained in the Indonesian Clinical Trials\\nGuidelines as the sources of diagnostic knowledge. The\\nretrieval module, which uses semantic similarity search-\\ning, then retrieves the relevant medical information based\\non the description of the symptoms of patient. A vector\\ndatabase has been developed, in which the processed\\nIndonesian Clinical Trials Guidelines content has served\\nas dense vectors to index the database to identify simi-\\nlarities. The generation part then integrates the retrieved\\nmedical information with the original patient case hence\\nproviding the LLM with the contextually relevant medical\\ninformation that helped in the generation of diagnostic\\nresponses.\\n\\nThe prompt that was used in the RAG was framed in\\nIndonesian in order to bring out consistency of evalu-\\nated models. The Indonesian Clinical Trials Guidelines\\n\\ndocument and the question for the sample medical case\\nitself is in Indonesian. Therefore, to generate a consistent\\nIndonesian answer, we utilized Indonesian prompt.\\n\\nD. Evaluation Techniques\\n\\nBefore we evaluate the candidate sentences, we first\\nnormalized them by matching them to a standardized\\nsynonyms that was listed on the synonym map. Therefore,\\nwords with similar meanings, but different wordings, such\\nas \\xe2\\x80\\x9cdemam dengue\\xe2\\x80\\x9d and \\xe2\\x80\\x9ddemam berdarah\\xe2\\x80\\x9d would be\\nstandardized to become *demam berdarah dengue\\xe2\\x80\\x9d.\\n\\nIn practice, *demam dengue\\xe2\\x80\\x9d and \\xe2\\x80\\x9d\\xe2\\x80\\x99demam berdarah\\ndengue\\xe2\\x80\\x9d are not the same, although they are both caused\\nby the dengue virus. The difference is in the laboratory\\nresults. Based on Indonesian Clinical Trials Guidelines,\\n*demam dengue\\xe2\\x80\\x9d can be a separate diagnosis due to\\nthe lack of \\xe2\\x80\\x99demam berdarah\\xe2\\x80\\x9d criteria. Meanwhile, a\\npatient is diagnosed to have \\xe2\\x80\\x9d*demam berdarah\\xe2\\x80\\x9d if they\\nalready satisfy the *demam berdarah\\xe2\\x80\\x9d criteria. However,\\nfor our current research, we focused on more generalized\\nresults that are easier to comprehend by the public, so we\\nstandardized them to be the same.\\n\\nNext, four metrics of evaluation were complemented to\\nmake a complete analysis of performance:\\n\\n\\xc2\\xab BERT (Bidirectional Encoder Representations from\\nTransformers) Score: Calculate the similarity score\\nbetween the candidate sentence and ground truth\\nbased on the sum of cosine similarities between\\nthe pre-trained BERT contextual embeddings of their\\ntokens [24].\\n\\n\\xc2\\xab BLEURT: a trained evaluation metric based on BERT\\nthat conveys human-level judgment of reference ver-\\nsus candidate sentence. BLEURT is not calibrated\\n(normalized), so we plotted a distribution graph based\\non BLEURT and used it to compare the model\\xe2\\x80\\x99s\\nperformance [25].\\n\\ne SAS (Semantic Answer Similarity): A cross-encoder-\\nbased evaluation metric that estimates the semantic\\nmeaning of the answer or candidate sentence and\\ncompare it with the reference sentence [22].\\n\\n\\xc2\\xab LLM-based Judge or G-Eval: prompt-based evaluator\\nthat utilizes the GPT LLM family as the judge\\nto simulate human quality judgment [23]. In our\\nevaluation process, we utilized the GPT-4-Turbo.\\n\\nE. Research Flow Overview\\n\\nThe research was conducted using an experimental\\nresearch design with a systematic procedure that compared\\nthe conditions of all models consistently. All four models\\nare presented with uniform RAG setups and the same\\nsample questions. However, we also include an additional\\nmodel, which is Deepseek without using RAG as a\\nbaseline to compare RAG and non-RAG LLM diagnostic\\ncapability. The instructions involved in carrying out the\\nassessments were also uniform, and the prompts were de-\\nsigned in a similar manner, so that the evaluation process\\nis considered \\xe2\\x80\\x99fair\\xe2\\x80\\x99 for all models. For the overview of our\\nresearch flow, we can see it in the flowchart in Figure 1.\\n'\n",
      "Page # 3 - b'TABLE I: Dataset\\xe2\\x80\\x99s Details\\n\\nColumns Description Data Type\\nNo the serial number of the medical case float\\nQuestion the medical case statement in the form of question object (string)\\nDr Answer the ground truth for that medical case object (string)\\n\\nClaude Answer\\n\\nQwen Answer\\n\\nGPT Answer\\n\\nDeepseek RAG Answer\\nDeepseek Non RAG Answer\\nClaude Full Answer\\n\\nQwen Full Answer\\n\\nGPT Full Answer\\n\\nDeepseek RAG Full Answer\\nDeepseek Non RAG Full Answer\\n\\nfull answer from Claude\\nfull answer from Qwen\\nfull answer from GPT\\n\\ndiseases diagnosed by Claude for that medical case\\n\\ndiseases diagnosed by Qwen for that medical case\\n\\ndiseases diagnosed by GPT for that medical case\\n\\ndiseases diagnosed by Deepseek for that medical case with RAG\\n\\ndiseases diagnosed by Deepseek for that medical case without using RAG\\n\\nfull answer from Deepseek using RAG\\nfull answer from Deepseek without using RAG\\n\\nobject (string)\\nobject (string)\\nobject (string)\\nobject (string)\\nobject (string)\\nobject (string)\\nobject (string)\\nobject (string)\\nobject (string)\\nobject (string)\\n\\nTABLE II: LLM Performance\\n\\nEvaluation Metrics Claude Qwen GPT _ Deepseek with RAG Deepseek without RAG\\nBERT\\xe2\\x80\\x99s Precision 0.860 0.814 0.820 0.812 0.791\\nBERT\\xe2\\x80\\x99s Recall 0.937 0.850 0.874 0.875 0.843\\nBERT\\xe2\\x80\\x99s Fl 0.896 0.831 0.846 0.842 0.816\\nSAS 0.962 0.494 0.688 0.653 0.414\\nLLM-based Judge 0.859 0.629 0.758 0.771 0.687\\n\\nGather documents\\n\\nIncorporate RAG to\\n\\n(PPPK) for RAG {+>} enhance model\\npreparation answers\\nGather answers\\nObtain sample\\n(diagnosis) from models questions (medical\\n\\nand ground truth from\\ndoctor\\n\\nNormalize model\\nanswers with\\nstandardized synonyms\\n\\ncases) from hospital\\n\\nEvaluate model\\n[+> answers using context\\nbased metrics\\n\\nFig. 1: Research Flow Flowchart\\n\\nIV. RESULTS AND DISCUSSION\\n\\nTable II shows the mean of the evaluation score for each\\nmodel when they are benchmarked with the medical case\\ndataset.\\n\\nFrom the table, we can see that overall, Claude\\xe2\\x80\\x99s perfor-\\nmance is the best, followed by GPT, Deepseek with RAG,\\nQwen, and lastly, Deepseek without RAG. The reason\\nwhy Claude has the best performance is highly related\\nto the ground truth establishment method that the medical\\npractitioner used. The doctor determined the ground truth\\nmostly by evaluating Claude\\xe2\\x80\\x99s diagnosis. Therefore, it is\\nnot surprising that Claude has the highest performance\\n\\namong all LLMs. Meanwhile, Deepseek without RAG is\\nas expected to rank as the last one, as it does not have\\nany additional context given by RAG.\\n\\nFrom the BERT scores, it can also be inferred that\\nall LLMs have better recall than precision in diagnosing\\ndiseases. In other words, the models tend to include many\\nrelevant (positive) terms in their diagnosis, but are inclined\\nto over-predicting positives. Hence, resulting in a lower\\nprecision score than the recall score. However, in medical\\ndiagnosis case, higher recall score is more favorable than\\nhigher precision score, because it is better to have a false\\npositive than a false negative, so that there is no disease\\nthat is missed in the diagnosis.\\n\\nNext, we explore the BERT F1 and LLM-based Judge\\nresult. As stated before, the order of LLMs based on their\\nperformance is Claude, GPT, Deepseek with RAG, Qwen,\\nthen Deepseek without RAG. BERT FI score reveals a\\nsimilar score for each model, implying that although some\\nmodels are better, the difference is not that significant.\\nThis conclusion is also supported by the LLM-based Judge\\nresult, where the scores for each model are close. The\\nscores are still in the range of 0.6 to 0.9, although the\\ndiscrepancy between each score is higher. However, as\\nwe can see in Table II, unlike other metrics, LLM-based\\nJudge gives a higher score to Deepseek without RAG than\\nQwen, even though Qwen is already enhanced with RAG.\\nA possible cause for this behaviour is due to the bias\\nor hallucination of the LLM judge, which gives a higher\\nscore to answers that match more with the evaluation\\nprompt.\\n\\nMeanwhile, SAS also has a similar pattern to BERT\\nF1, but shows a greater difference between each model\\xe2\\x80\\x99s\\nscore. Claude has a relatively high score, while models\\nsuch as Qwen and Deepseek without RAG obtained a\\nlow score of 0.4. This distinction indicates that cross-\\nencoder approach that is used in SAS evaluation metrics\\n'\n",
      "Page # 4 - b'Distribution of BLEURT Scores\\n\\nClaude BLEURT\\n\\nQwen_BLEURT\\n\\n0.75 -0150 0.25 0.00 0.25 050 075\\nGPT_BLEURT\\n\\nol\\n\\xe2\\x80\\x9c150 -1.25\\n\\n\\xe2\\x80\\x9c100-075-050 0.25 0.00 025\\nDeepseek RAG_BLEURT\\n\\n\\xe2\\x80\\x9c100-075-050 0.25\\nDeepseek non RAG_BLEURT\\n\\n0.75 ~0.50 0.25 0.00 0.25\\n\\n12 \\xe2\\x80\\x9c10 0.8 0.6 0.4 0.2 a0 02\\n\\nFig. 2: BLEURT Distribution Histogram for Each LLM\\n\\nimpose stricter criteria when evaluating similarity. Cross-\\nencoders process sentence pairs jointly to capture the\\ncomplex interaction between them, thereby making them\\nmore sensitive when penalizing subtle mismatches. As a\\nresult, the score discrepancy is higher.\\n\\nBesides scores such as BERT, SAS, and LLM-based\\nJudge, we also evaluate each model\\xe2\\x80\\x99s performance with\\nBLEURT. However, as stated in the Methodology part,\\nthe BLEURT score is not normalized, so we plotted the\\nhistogram to visualize the distribution of the BLEURT\\nscore of each model\\xe2\\x80\\x99s diagnosis for each question. The\\nhistogram can be seen in Figure 2.\\n\\nThe histograms in Figure 2 reveal the similar outcome\\nas the other evaluation metrics. The more left-skewed\\na distribution, the better the result is because it means\\nthat there are many positive scores. From the histograms,\\nwe can see that Claude has the best result because the\\ndistribution is skewed towards a positive score. Then, we\\nhave GPT which is the second best, it is left skewed,\\nbut still produces a high negative score (about -1.50).\\nNext are Deepseek with RAG and Qwen. Both histograms\\nare similar, but Deepseek with RAG is slightly better\\nthan Qwen because it does not have a strongly negative\\nvalue. Lastly, there is Deepseek without using RAG. The\\nhistogram shows that the model prediction is unstable,\\nwith some predictions being good (positive score), but\\nsome are strongly negative. This result demonstrated that\\nRAG can also improve the stability of the answers for\\nLLMs.\\n\\nV. CONCLUSION AND FUTURE WORK\\n\\nIn conclusion, this study analyzes the medical diagno-\\nsis performance of each popular LLM by collaborating\\nwith a medical practitioner (doctor) for the ground truth\\n\\nestablishment, then uses it as a base to benchmark each\\nmodel. Each model is also enhanced by RAG using the\\nPanduan Praktik Profesional Kedokteran or Indonesian\\nClinical Trials Guidelines to provide context related to\\ndisease symptoms and diagnosis. Then, we evaluate each\\nmodel using context-based metrics.\\n\\nAfter reviewing the metrics, the result showed that the\\norder of models based on their diagnosis performance\\nis Claude, GPT, Deepseek with RAG, Qwen, and then\\nfollowed by Deepseek without RAG. Claude has the\\nbest performance because the doctor used it as the main\\nevaluation reference. Meanwhile, Deepseek without RAG\\nscored the lowest because it is not supported by the context\\ngiven in RAG.\\n\\nLooking ahead, future research could focus on eval-\\nuating more diverse models and datasets. We plan to\\ngather more data to be used as the medical cases in\\nthe benchmark dataset. Besides that, collaborating with\\nmore medical practitioners to establish the ground truth\\nfor each medical case is also beneficial, so that we can\\nobtain a more representative ground truth. Choosing a\\nspecific medical dataset for evaluation, such as a dataset\\nthat only contains heart diseases, can also be used to\\nbenchmark each model to determine which model is the\\nbest in diagnosing diseases in a specific medical field.\\nAdditionally, we can also incorporate more evaluation\\nmetrics to provide a more comprehensive view of each\\nmodel\\xe2\\x80\\x99s performance, or more LLMs to be tested, so that\\nwe can see their performance.\\n\\nSUPPLEMENTARY CODES\\n\\nAll codes that were used in our research can be\\naccessed in our GitHub repository through the link:\\nhttps://github.com/Ella-Raputri/LLMDiagnosisPPPK.\\n'\n",
      "Page # 5 - b'CONTRIBUTORSHIP\\n\\nElla Raputri: Methodology, Software, Validation, For-\\nmal Analysis, Investigation, Data Curation, Writing -\\nOriginal Draft, Visualization. Ari Jaya Teguh: Method-\\nology, Software, Investigation, Writing - Original Draft.\\nSaffanah Nur Hidayah: Validation, Resources. Nunung\\nNurul Qomariyah: Conceptualization, Writing - Review\\n& Editing, Supervision, Project Administration. Feri Se-\\ntiawan: Conceptualization, Writing - Review & Editing,\\nSupervision, Project Administration.\\n\\n(\\n\\n[2]\\n\\n(3)\\n\\n[4]\\n\\nREFERENCES\\n\\nS. Paliwal, V. Bharti, and A. K. Mishra, \\xe2\\x80\\x9cAi chatbots: Transforming\\nthe digital world,\\xe2\\x80\\x99 in Recent trends and advances in artificial\\nintelligence and internet of things. Springer, 2019, pp. 455-482.\\nF. Sun, \\xe2\\x80\\x9cChatgpt, the start of a new era,\\xe2\\x80\\x99 A Bright and Gloomy Fu-\\nture. In https://feisun. org/2022/12/23/-chatgpt-the-start-of-a-new-\\nera (ultima consultazione: 27/03/2023), 2022.\\n\\nH. Taherdoost and M. Madanchian, \\xe2\\x80\\x9cArtificial intelligence and\\nknowledge management: Impacts, benefits, and implementation,\\xe2\\x80\\x9d\\nComputers, vol. 12, no. 4, p. 72, 2023.\\n\\nM.-H. Temsah, F. Aljamaan, K. H. Malki, K. Alhasan, I. Altamimi,\\nR. Aljarbou, F. Bazuhair, A. Alsubaihin, N. Abdulmajeed,\\nF. S. Alshahrani, R. Temsah, T. Alshahrani, L. Al-Eyadhy,\\nS. M. Alkhateeb, B. Saddik, R. Halwani, A. Jamal, J. A.\\nAl-Tawfiq, and A. Al-Eyadhy, \\xe2\\x80\\x9cChatgpt and the future of\\ndigital health: A study on healthcare workers\\xe2\\x80\\x99 perceptions and\\nexpectations,\\xe2\\x80\\x9d Healthcare, vol. 11, no. 13, 2023. [Online].\\nAvailable: https://www.mdpi.com/2227-9032/1 1/13/1812\\nI. Altamimi, A. Altamimi, A. S. Alhumimidi, A. Altamimi, and\\nM.-H. Temsah, \\xe2\\x80\\x9cArtificial intelligence (ai) chatbots in medicine:\\nA supplement, not a substitute,\\xe2\\x80\\x9d Cureus, Jun. 2023. [Online].\\nAvailable: http://dx.doi.org/10.7759/cureus.40922\\nS. Reddy, W. Rogers, V.-P. Makinen, E. Coiera, P. Brown,\\nM. Wenzel, E. Weicken, S. Ansari, P. Mathur, A. Casey, and\\nB. Kelly, \\xe2\\x80\\x9cEvaluation framework to guide implementation of\\nai systems into healthcare settings,\\xe2\\x80\\x99 BMJ Health amp; Care\\nInformatics, vol. 28, no. 1, p. e100444, Oct. 2021. [Online].\\nAvailable: http://dx.doi.org/10.1136/bmjhci-2021-100444\\nZ. Ji, T. Yu, Y. Xu, N. Lee, E. Ishii, and P. Fung, \\xe2\\x80\\x9cTowards\\nmitigating LLM hallucination via self reflection,\\xe2\\x80\\x9d in Finding:\\nof the Association for Computational Linguistics: EMNLP 2023,\\nH. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for\\nComputational Linguistics, Dec. 2023, pp. 1827-1843. [Online].\\nAvailable: https://aclanthology.org/2023.findings-emnlp. 123/\\n\\nS. Zhou, Z. Xu, M. Zhang, C. Xu, Y. Guo, Z. Zhan, Y. Fang,\\nS. Ding, J. Wang, K. Xu, L. Xia, J. Yeung, D. Zha, D. Cai, G. B.\\nMelton, M. Lin, and R. Zhang, \\xe2\\x80\\x9cLarge language models for disease\\ndiagnosis: a scoping review,\\xe2\\x80\\x9d npj Artificial Intelligence, vol. 1, 6\\n2025.\\n\\nD. Wang and S. Zhang, \\xe2\\x80\\x9cLarge language models in medical and\\nhealthcare fields: applications, advances, and challenges,\\xe2\\x80\\x9d Artificial\\nIntelligence Review, vol. 57, 11 2024.\\n\\nE. Goh, R. Gallo, J. Hom, E. Strong, Y. Weng, H. Kerman, J. A.\\nCool, Z. Kanjee, A. S. Parsons, N. Ahuja, E. Horvitz, D. Yang,\\nA. Milstein, A. P. Olson, A. Rodman, and J. H. Chen, \\xe2\\x80\\x9cLarge\\nlanguage model influence on diagnostic reasoning: A randomized\\nclinical trial?\\xe2\\x80\\x99 JAMA Network Open, vol. 7, 10 2024.\\n\\nA. Rios-Hoyo, N. L. Shan, A. Li, A. T. Pearson, L. Pusztai, and\\nF.M. Howard, \\xe2\\x80\\x9cEvaluation of large language models as a diagnostic\\naid for complex medical cases,\\xe2\\x80\\x9d Frontiers in Medicine, vol. 11,\\n2024.\\n\\nX. Meng, X. Yan, K. Zhang, D. Liu, X. Cui, Y. Yang, M. Zhang,\\nC. Cao, J. Wang, X. Wang, J. Gao, Y. G. S. Wang, J. ming Ji,\\nZ. Qiu, M. Li, C. Qian, T. Guo, S. Ma, Z. Wang, Z. Guo, Y. Lei,\\nC. Shao, W. Wang, H. Fan, and Y. D. Tang, \\xe2\\x80\\x9cThe application of\\nlarge language models in medicine: A scoping review,\\xe2\\x80\\x9d iScience,\\nvol. 27, 5 2024.\\n\\nW. H. K. Chiu, W. S. K. Ko, W. C. S. Cho, S. Y. J. Hui, W. C. L.\\nChan, and M. D. Kuo, \\xe2\\x80\\x9cEvaluating the diagnostic performance\\nof large language models on complex multimodal medical cases,\\xe2\\x80\\x9d\\nJournal of Medical Internet Research, vol. 26, 2024.\\n\\nO. K. Gargari and G. Habibi, \\xe2\\x80\\x9cEnhancing medical ai with retrieval-\\naugmented generation: A mini narrative review,\\xe2\\x80\\x9d 1 2025.\\n\\n20\\n\\n21\\n\\n22)\\n\\n[23]\\n\\n[24]\\n\\n[25]\\n\\nfor\\n\\nfor\\n\\nR. Yang, Y. Ning, E. Keppo, M. Liu, C. Hong, D. S. Bitterman,\\nJ.C. L. Ong, D. S. W. Ting, and N. Liu, \\xe2\\x80\\x9cRetrieval-augmented\\ngeneration for generative artificial intelligence in health care,\\xe2\\x80\\x9d npj\\nHealth Systems, vol. 2, 1 2025.\\n\\nJ. Miao, C. Thongprayoon, S. Suppadungsuk, O. A. G. Valencia,\\nand W. Cheungpasitporn, \\xe2\\x80\\x9cIntegrating retrieval-augmented genera-\\ntion with large language models in nephrology: Advancing practical\\napplications,\\xe2\\x80\\x9d 3 2024.\\n\\nA. Bora and H. Cuaydhuitl, \\xe2\\x80\\x9cSystematic analysis of retrieval-\\naugmented generation-based Ilms for medical chatbot applications,\\xe2\\x80\\x9d\\nMachine Learning and Knowledge Extraction, vol. 6, pp. 2355\\xe2\\x80\\x94\\n2374, 12 2024.\\n\\nG. Xiong, Q. Jin, Z. Lu, and A. Zhang, \\xe2\\x80\\x9cBenchmarking\\nretrieval-augmented generation for medicine,\\xe2\\x80\\x99 Association\\nComputational Linguistics, pp. 6233-6251, 8 2024.\\n[Online]. Available: https://github.com/Teddy-XiongGZ/MedRAG\\nhttps://aclanthology.org/2024.findings-acl.372/\\n\\nM. Alkhalaf, P. Yu, M. Yin, and C. Deng, \\xe2\\x80\\x9cApplying generative\\nai with retrieval augmented generation to summarize and extract\\nkey clinical information from electronic health records,\\xe2\\x80\\x9d Journal\\nof Biomedical Informatics, vol. 156, 8 2024.\\n\\nT. Sellam, D. Das, and A. P. Parikh, \\xe2\\x80\\x9cBleurt: Learning robust\\nmetrics for text generation,\\xe2\\x80\\x99 Association for Computational\\nLinguistics, pp. 7881-7892, 6 2020. [Online]. Available:\\nhttp://github.com/google-research/\\n\\nT. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi,\\n\\xe2\\x80\\x9cBertscore: Evaluating text generation with bert,\\xe2\\x80\\x99 Jnternational\\nConference on Learning Representations, 2 2020. [Online].\\nAvailable: http://arxiv.org/abs/1904.09675\\n\\nJ. Risch, T. Mller, J. Gutsch, and M. Pietsch, \\xe2\\x80\\x9cSemantic\\nanswer similarity for evaluating question answering models,\\xe2\\x80\\x9d\\nin Proceedings of the 3rd Workshop on Machine Reading\\nQuestion Answering, A. Fisch, A. Talmor, D. Chen,\\nE. Choi, M. Seo, P. Lewis, R. Jia, and S. Min, Eds.\\nPunta Cana, Dominican Republic: Association for Computational\\nLinguistics, Nov. 2021, pp. 149-157. [Online]. Available:\\nhttps://aclanthology.org/2021.mrqa-1.15/\\n\\nY. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu,\\n\\xe2\\x80\\x9cG-eval: NLG evaluation using gpt-4 with better human\\nalignment,\\xe2\\x80\\x9d in Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, H. Bouamor, J. Pino,\\nand K. Bali, Eds. Singapore: Association for Computational\\nLinguistics, Dec. 2023, pp. 2511-2522. [Online]. Available:\\nhttps://aclanthology.org/2023.emnlp-main. 153/\\n\\nT. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi,\\n\\xe2\\x80\\x9cBertscore: Evaluating text generation with bert,\\xe2\\x80\\x9d in Jnternational\\nConference on Learning Representations, 2020. [Online].\\nAvailable: https://openreview.net/forum?id=SkeHuC VFDr\\n\\nT. Sellam, D. Das, and A. P. Parikh, \\xe2\\x80\\x9cBleurt: Learning robust\\nmetrics for text generation,\\xe2\\x80\\x9d in Proceedings of ACL, 2020.\\n'\n"
     ]
    }
   ],
   "source": [
    "for page_number, page_data in enumerate(doc):\n",
    "    txt = pytesseract.image_to_string(page_data).encode(\"utf-8\")\n",
    "    print(\"Page # {} - {}\".format(str(page_number),txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23de8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3422 Journal on Education, Volume 07, No. 01, September-Desember 2024, hal. 3418-3425\n",
      "\n",
      "karena terdapat dua kata yang digabungkan yaitu kata jalur dan pribadi. “JP” sendiri berasal dari\n",
      "kalimat “JALUR PRIBADI” yang biasanya digunakan oleh remaja untuk berkomunikasi sehari-hari\n",
      "baik secara langsung maupun dalam media sosial.\n",
      "\n",
      "Lop=Love; U=kamu; Org=Orang\n",
      "\n",
      "buyonegetone @xxvnyyxx - 28 Jun 23\n",
      "hahaha lop u buat 1 org\n",
      "\n",
      "QO ban iw] ht 59 As\n",
      "\n",
      "Gambar 5. Bahasa Gaul 5\n",
      "\n",
      "Postingan tersebut terdapat bahasa gaul yang ditemukan dari hasil pengamatan dalam media\n",
      "sosial Twitter di atas. Berdasarkan dari pengamatan tersebut ada beberapa bahasa gaul dalam bentuk\n",
      "kata “LOP” yang terbentuk dari kata dasar “LOVE”, yang dimana kata tersebut berasal dari bahasa\n",
      "asing. Kata “lop” juga merupakan bahasa gaul dengan mengubah suku kata yang seharusnya love\n",
      "menjadi “lop.”\n",
      "\n",
      "Terdapat istilah “U” merupakan bahasa gaul dari kata”>KAMU” kemudian dipersingkat dengan\n",
      "menghilangkan fonem /k/,/a/, dan /m/ sehingga hanya terdapat huruf “U” saja. Bahasa gaul tersebut\n",
      "banyak digunakan dalam komunikasi banyak pengguna media sosial.\n",
      "\n",
      "Kemudan terdapat juga bahasa gaul yaitu berupa singkatan “ORG” yang seharusnya “ORANG”\n",
      "disingkat menjadi “org” dengan menghilangkan fonem /a/, dan fonem /n pada tengah kata.\n",
      "\n",
      "Ga=Tidak; Nurul=Nalar\n",
      "\n",
      "buyonegetone @xxvnyyxx - 01 Jul 23\n",
      "ga berharap lebih dengan nilai cak nurul\n",
      "1@) a 1) it 38 Ns\n",
      "\n",
      "Gambar 6. Bahasa Gaul 6\n",
      "Pada postingan tersebut terdapat bahasa gaul, berdasarkan penelitian yang telah dilakukan\n",
      "terdapat Bahasa gaul dengan kata GA” dan “NURUL”. Kata “GA” merupakan istilah singkatan yang\n",
      "sering digunakan dalam melakukan komunikasi media sosial. Istilah tersebut merupakan kata lain dari\n",
      "“TIDAK” yang biasa disebutkan dengan enggak, sehingga untuk lebih mempersingkatnya menjadi\n",
      "”ga” dengan menghilangkan fonem/e/, /n/, /g/ pada awal kata dan fonem /k/ pada akhir kata.\n",
      "Kemudian terdapat bahasa gaul berupa kata *\"NURUL” dimana kata gaul tersebut termasuk\n",
      "slang dan merupakan plesetan dari kata dasar \"NALAR”. Istilah tersebut populer hingga kini di\n",
      "kalangan remaja.\n",
      "Penelitian ini juga membahas mengenai pengaruh dari penggunaan bahasa gaul pada media\n",
      "sosial di kalangan remaja. Terdapat dua pengaruh yang ditimbulkan dari bahasa gaul yaitu dampak\n",
      "positif dan dampak negatif. Selain itu, Beta Puspa (2015 : 5) juga mengungkapkan bahwa ditemukan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt = pytesseract.image_to_string(Image.open('../data/raw/source_file/101.png'))\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e354773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open('../data/raw/source_file/101.png')\n",
    "\n",
    "data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f482ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('test_101.json','w') as file:\n",
    "    json.dump(data, file, indent=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
