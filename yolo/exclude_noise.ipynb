{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dac5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35edd671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Research-Methodology\\testenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import json\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d3d0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "d:\\Research-Methodology\\testenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--Salesforce--blip-image-captioning-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\Research-Methodology\\testenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-id. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\Research-Methodology\\testenv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "translator = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-id\")\n",
    "translator_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09dc536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img_url, img_obj):\n",
    "    ymin, ymax, xmin, xmax = img_obj[\"y_min\"], img_obj[\"y_max\"], img_obj[\"x_min\"], img_obj[\"x_max\"]\n",
    "    raw_image = Image.open(img_url).convert('RGB')\n",
    "    crop = raw_image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    text = \"an image of\"\n",
    "    inputs = caption_processor(crop, text, return_tensors=\"pt\")\n",
    "\n",
    "    out = caption_model.generate(**inputs)\n",
    "    caption_en = caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    translated = translator.generate(**translator_tokenizer(caption_en, return_tensors=\"pt\", padding=True))\n",
    "    caption_id = translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    print(caption_id, caption_en, sep='\\n')\n",
    "    return caption_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1538baaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\Research-Methodology\\yolo\\..\\data\\raw\\source_file\\504.png: 640x480 5 Captions, 2 PageNumbers, 2 Pictures, 267.4ms\n",
      "Speed: 8.6ms preprocess, 267.4ms inference, 18.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mD:\\Research-Methodology\\yolo\\detect_result\\predict2\u001b[0m\n",
      "1 label saved to D:\\Research-Methodology\\yolo\\detect_result\\predict2\\labels\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"models/kfold_result/kfold_training/fold_4/weights/best.pt\")\n",
    "results = model.predict(\n",
    "    source=\"../data/raw/source_file/504.png\", save=True, save_txt=True, save_conf=True,\n",
    "    project=\"detect_result\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2584d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file_names = []\n",
    "img_json = []\n",
    "id2label = {0: \"Caption\", 1:\"PageNumber\", 2:\"Picture\"}\n",
    "\n",
    "for i in range(504, 505):\n",
    "    name = f'../data/raw/source_file/{i}.png'\n",
    "    data = f'../data/raw/ocr_dict/dict_{i}.json'\n",
    "    img_file_names.append(name)\n",
    "    img_json.append(data)\n",
    "\n",
    "label_dir = 'detect_result/predict/labels' #yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_result(image_path, label_path):\n",
    "    detected = {i : list() for i in range(0, 3)}\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # print(f\"Image: {image_name}\")\n",
    "    for line in lines:\n",
    "        # print(line, end='')\n",
    "        label, x_center, y_center, w, h, confidence = map(float, line.strip().split())\n",
    "\n",
    "        x_min = int((x_center - w / 2) * img_width)\n",
    "        y_min = int((y_center - h / 2) * img_height)\n",
    "        x_max = int((x_center + w / 2) * img_width)\n",
    "        y_max = int((y_center + h / 2) * img_height)\n",
    "\n",
    "        obj = dict()\n",
    "        obj['x_min'] = x_min\n",
    "        obj['x_max'] = x_max\n",
    "        obj['y_min'] = y_min\n",
    "        obj['y_max'] = y_max\n",
    "        obj['conf'] = confidence\n",
    "        detected[label].append(obj)\n",
    "    \n",
    "    return detected\n",
    "\n",
    "        #     print(f\"Class {id2label[int(label)]}: (x_min={x_min}, y_min={y_min}), (x_max={x_max}, y_max={y_max})\")\n",
    "        #     print(f\"Image width: {img_width}, image height: {img_height}\")\n",
    "        #     print()\n",
    "        # print(\"=====================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_rect(image_obj, x_min2, x_max2, y_min2, y_max2):\n",
    "    tolerance = 2\n",
    "    x_min1, x_max1 = image_obj['x_min']+tolerance, image_obj['x_max']+tolerance\n",
    "    y_min1, y_max1 = image_obj['y_min']+tolerance, image_obj['y_max']+tolerance\n",
    "\n",
    "    return not ((x_max1 <= x_min2 or x_max2 <= x_min1) or\n",
    "                (y_max1 <= y_min2 or y_max2 <= y_min1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(ocr_result, detected, caption_pos):\n",
    "    new_txt = \"\"\n",
    "    max_idx = len(ocr_result['text'])\n",
    "\n",
    "    # tdk append text dalam image yg kedetect ocr\n",
    "    for i in range(max_idx):\n",
    "        text = ocr_result['text'][i]\n",
    "        if i in caption_pos: new_txt += text + \" \"\n",
    "        \n",
    "        is_exclude = False\n",
    "        exclude_list = detected[1] + detected[2] #1:pagenumber, 2:picture\n",
    "\n",
    "        for obj in exclude_list:\n",
    "            # print('masyk')\n",
    "            x_min2 = ocr_result['left'][i]\n",
    "            x_max2 = ocr_result['left'][i]+ocr_result['width'][i]\n",
    "            y_min2 = ocr_result['top'][i]\n",
    "            y_max2 = ocr_result['top'][i]+ocr_result['height'][i]\n",
    "\n",
    "            if(intersect_rect(obj, x_min2, x_max2, y_min2, y_max2)): \n",
    "                is_exclude = True\n",
    "                break\n",
    "        \n",
    "        if not is_exclude and text != \"\": new_txt += text + \" \"\n",
    "    \n",
    "    return new_txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bcaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_caption_img(detected):\n",
    "    if len(detected[0]) == len(detected[2]): #kalau sama jumlahnya\n",
    "        return []\n",
    "    \n",
    "    caption_img = [] #{caption: no urut caption, img: no urut img}: dict\n",
    "    img_used = {} #utk track img apakah udah dipakai atau belum\n",
    "\n",
    "    # loop greedy matching caption-img\n",
    "    for id, caption in enumerate(detected[0]):\n",
    "        y_min_c, y_max_c = caption['y_min'], caption['y_max']\n",
    "        min_val = 1e5\n",
    "        ambil_idx = -1\n",
    "        ambil_obj = None\n",
    "\n",
    "        for idx, img in enumerate(detected[2]):\n",
    "            y_min_img, y_max_img = img['y_min'], img['y_max']\n",
    "\n",
    "            # caption di atas img\n",
    "            diff_up = abs(y_min_img-y_max_c)\n",
    "            # kalau caption di bawah img\n",
    "            diff_down = abs(y_min_c - y_max_img)\n",
    "            \n",
    "            if(diff_up < diff_down): \n",
    "                if diff_up < min_val: \n",
    "                    min_val = diff_up\n",
    "                    ambil_idx = idx\n",
    "                    ambil_obj = img\n",
    "            else: \n",
    "                if diff_down < min_val: \n",
    "                    min_val = diff_down\n",
    "                    ambil_idx = idx\n",
    "                    ambil_obj = img\n",
    "        \n",
    "        # print(id, min_val, ambil_idx)\n",
    "\n",
    "        #caption lebih byk dari img\n",
    "        if(ambil_idx in img_used):\n",
    "            if(min_val < img_used[ambil_idx]['value']):\n",
    "                prev_caption_idx = img_used[ambil_idx]['caption']\n",
    "                caption_img[prev_caption_idx]['img'] = None\n",
    "                caption_img[prev_caption_idx]['img_obj'] = None\n",
    "        \n",
    "        caption_img.append({\"caption\":id, \"img\":ambil_idx, \"img_obj\":ambil_obj}) \n",
    "        img_used[ambil_idx] = {\"caption\":id, \"value\":min_val}\n",
    "\n",
    "    # img lbih byk dri caption\n",
    "    for image_id, image in enumerate(detected[2]):\n",
    "        if image_id not in img_used:\n",
    "            caption_img.append({\"caption\":None, \"img\":image_id, \"img_obj\":image}) \n",
    "\n",
    "    # cleaning, utamain img\n",
    "    caption_img = [record for record in caption_img if record[\"img\"] is not None]\n",
    "    return caption_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d61ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_caption(record, ocr_data, excludes):\n",
    "    caption_x = (record['img_obj']['x_min'] + record['img_obj']['x_max']) / 2\n",
    "    caption_y = (record['img_obj']['y_min'] + record['img_obj']['y_max']) / 2\n",
    "    caption_str = record['caption']\n",
    "\n",
    "    min_dist = 1e5\n",
    "    insert_index = -1\n",
    "\n",
    "    for i in range(len(ocr_data['top'])):\n",
    "        if(ocr_data['left'][i] > caption_x) or (ocr_data['top'][i] > caption_y): continue\n",
    "        if i in excludes: continue\n",
    "        \n",
    "        # hitung manhattan\n",
    "        manhattan = abs(ocr_data['top'][i]-caption_y) + abs(ocr_data['left'][i]-caption_x)\n",
    "        if (manhattan <= min_dist):\n",
    "            min_dist = manhattan\n",
    "            insert_index = i+1\n",
    "\n",
    "    excludes.append(insert_index)\n",
    "    ocr_data['left'].insert(insert_index, record['img_obj']['x_max']+1)\n",
    "    ocr_data['top'].insert(insert_index, record['img_obj']['y_max']+1)\n",
    "\n",
    "    ocr_data['width'].insert(insert_index, abs(record['img_obj']['x_max']-record['img_obj']['x_min']))\n",
    "    ocr_data['height'].insert(insert_index, abs(record['img_obj']['y_max']-record['img_obj']['y_min']))\n",
    "    ocr_data['text'].insert(insert_index, caption_str)\n",
    "\n",
    "    return ocr_data, excludes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "final_txts = [] \n",
    "\n",
    "for i, dict_path in enumerate(img_json):\n",
    "    label_path = f\"{label_dir}/{i+1}.txt\"\n",
    "    detected_objs = get_detected_result(img_file_names[i], label_path)\n",
    "\n",
    "    with open(f'{dict_path}','r') as file:\n",
    "        ocr_data = json.load(file)\n",
    "\n",
    "    matches = match_caption_img(detected_objs)\n",
    "    excludes = []\n",
    "\n",
    "    for record in matches:\n",
    "        if record['img'] and not record['caption']:\n",
    "            new_caption = generate_caption(img_file_names[i], record['img_obj'])\n",
    "            # ubah caption-img (matches) ke dict \n",
    "            # add caption ke cleaned text\n",
    "            record['caption'] = new_caption\n",
    "            ocr_data, excludes = append_caption(record, ocr_data, excludes)\n",
    "\n",
    "    cleaned = clean_text(ocr_data, detected_objs, excludes)\n",
    "    final_txts.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b46f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
