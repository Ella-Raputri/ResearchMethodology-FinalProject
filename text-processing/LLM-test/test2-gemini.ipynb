{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c05bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAKwDHSHu8X_ALcjp9mR-6zwZNyXm5eP1I\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.genai import types\n",
    "from google import genai\n",
    "import re, os, json\n",
    "from jiwer import wer, cer\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import process\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "print(API_KEY)\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb821973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 ../data/raw/ocr_result/ocr_176.txt ../data/raw/ocr_result/ocr_192.txt\n",
      "17 ../data/raw/ground_truth/ocr_176.txt ../data/raw/ground_truth/ocr_192.txt\n"
     ]
    }
   ],
   "source": [
    "start = 176\n",
    "n = 193\n",
    "text_list = [f'../data/raw/ocr_result/ocr_{x}.txt' for x in range(start, n)]\n",
    "print(len(text_list), text_list[0], text_list[-1])\n",
    "\n",
    "gt_list = [f'../data/raw/ground_truth/ocr_{x}.txt' for x in range(start, n)]\n",
    "print(len(gt_list), gt_list[0], gt_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406a992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt(row):\n",
    "    prompt = (\n",
    "        \"Teks berikut adalah hasil OCR:\\n\\n\"\n",
    "        f\"{row}\\n\\n\"\n",
    "        \"Instruksi:\\n\"\n",
    "        \"1. Susun ulang teks agar urut dan mudah dibaca.\\n\"\n",
    "        \"2. Perbaiki typo dan kesalahan pemenggalan kata.\\n\"\n",
    "        \"3. Jangan menambah, mengurangi, atau mengubah informasi apa pun.\\n\"\n",
    "        \"4. Jangan menambah komentar, penjelasan, atau catatan.\\n\"\n",
    "        \"5. Output harus berupa teks final saja, tanpa markdown, tanpa format tambahan.\\n\"\n",
    "        \"6. Jangan melakukan asumsi atau mengisi bagian teks yang hilang.\\n\"\n",
    "        \"7. Teks output **harus berisi kata-kata yang sama dengan input**, kecuali kata yang memang diperbaiki karena typo.\\n\"\n",
    "        \"8. Jangan mengubah struktur kalimat secara berlebihan—hanya rapikan urutan dan perbaiki ejaan.\\n\\n\"\n",
    "        \"PERINTAH PENTING:\\n\"\n",
    "        \"• Jangan membuat kalimat baru.\\n\"\n",
    "        \"• Jangan menghilangkan kata.\\n\"\n",
    "        \"• Jangan melakukan halusinasi.\\n\"\n",
    "        \"• Kembalikan hanya teks yang sudah diperbaiki.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2216b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process\n",
    "from scipy.stats import kendalltau\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def split_into_units(text):\n",
    "    text = text.replace(\"\\r\", \"\").strip()\n",
    "    text = re.sub(r\"(\\S+@\\S+)\\.(\\S+)\", r\"\\1<dot>\\2\", text)\n",
    "\n",
    "    protected = [\n",
    "        \"Vol.\", \"No.\", \"hlm.\", \"eISSN.\", \"p-ISSN.\", \"etc.\"\n",
    "    ]\n",
    "    for p in protected:\n",
    "        text = text.replace(p, p.replace(\".\", \"<dot>\"))\n",
    "    \n",
    "    text = text.replace(\"\\n\", \"<nl>\")\n",
    "\n",
    "    parts = re.split(r\"\\.\\s+(?=[A-Z])\", text)\n",
    "    parts = [p.replace(\"<dot>\", \".\") for p in parts]\n",
    "    parts = [p.replace(\"<nl>\", \" \") for p in parts]\n",
    "    units = [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "    return units\n",
    "\n",
    "\n",
    "def kendall_tau_sentence_level(gt_text, pred_text):\n",
    "    gt_units = split_into_units(gt_text)\n",
    "    pred_units = split_into_units(pred_text)\n",
    "    print(gt_units)\n",
    "    print(pred_units)\n",
    "\n",
    "    gt_order = list(range(len(gt_units)))\n",
    "    matched_indices = []\n",
    "    for unit in pred_units:\n",
    "        match = process.extractOne(unit, gt_units)\n",
    "        if match is None:\n",
    "            continue\n",
    "        matched_indices.append(match[2])  \n",
    "\n",
    "    unique_pred_order = []\n",
    "    seen = set()\n",
    "    for idx in matched_indices:\n",
    "        if idx not in seen:\n",
    "            unique_pred_order.append(idx)\n",
    "            seen.add(idx)\n",
    "\n",
    "    min_len = min(len(gt_order), len(unique_pred_order))\n",
    "    gt_order = gt_order[:min_len]\n",
    "    unique_pred_order = unique_pred_order[:min_len]\n",
    "\n",
    "    if min_len < 2:\n",
    "        return 0\n",
    "\n",
    "    tau, p = kendalltau(gt_order, unique_pred_order)\n",
    "    return tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bf8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = []\n",
    "trained = []\n",
    "\n",
    "def pmr(gt, pred):\n",
    "    gt_words = gt.split()\n",
    "    pred_words = pred.split()\n",
    "    length = min(len(gt_words), len(pred_words))\n",
    "    matches = sum(1 for i in range(length) if gt_words[i] == pred_words[i])\n",
    "    return matches / length\n",
    "\n",
    "\n",
    "def evaluate(gt, res, isBaseline):\n",
    "    wer_ = wer(gt, res)\n",
    "    cer_ = cer(gt, res)\n",
    "    pmr_ = pmr(gt, res)\n",
    "    tau_ = kendall_tau_sentence_level(gt, res)\n",
    "\n",
    "    if isBaseline:\n",
    "        baseline.append({\"wer\": wer_, \"cer\": cer_, \"pmr\": pmr_, \"tau\": tau_})\n",
    "    else:\n",
    "        trained.append({\"wer\": wer_, \"cer\": cer_, \"pmr\": pmr_, \"tau\": tau_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ec4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661\n",
      "Harini et al., Peran Integrasi Artificial Intelligence dalam Pembelajaran Digital ... 2610\n",
      "\n",
      "dianalisis secara kualitatif dengan cara membaca secara menyeluruh, menandai temuan-temuan utama, serta mengidentifikasi keterkaitan antara integrasi AI, efektivitas kognitif, dan kreativitas dalam konteks pembelajaran digital. Data dianalisis menggunakan teknik content analysis, dengan memfokuskan perhatian pada tema-tema utama, model integrasi teknologi, serta pengaruhnya terhadap proses dan hasil belajar mahasiswa. Hasil dari analisis tersebut disajikan dalam bentuk uraian naratif yang sistematis untuk menggambarkan kontribusi AI dalam pembelajaran digital serta implikasinya terhadap pengembangan kompetensi mahasiswa di era teknologi.\n",
      "\n",
      "Studi Tinjauan Pustaka\n",
      "Pengumpulan Data Artikel Berbasis Kriteria\n",
      "Seleksi Data Kualitatif Menggunakan Limitasi Inklusif\n",
      "Analisis Kualitatif\n",
      "Pendekatan Sumber dari Google Scholar dan situs web bereputasi kredibel lainnya (1983-2025)\n",
      "+ Dipublikasikan di jurnal\n",
      "+ Telaah integrasi AI dan dampaknya terhadap efektivitas kognitif dan kreativitas dalam pembelajaran digital\n",
      "+ Eksplorasi peran integrasi AI dalam pembelajaran digital, efektivitas kognitif, dan kreativitas mahasiswa\n",
      "+ Pendekatan empiris atau pengaruhnya\n",
      "\n",
      "Gambar 1. Diagram alur penelitian\n",
      "\n",
      "HASIL\n",
      "\n",
      "Artificial Intelligence (AI) merujuk pada kemampuan sistem komputer untuk melakukan tugas-tugas yang umumnya memerlukan kecerdasan manusia, seperti mengenali pola, membuat keputusan, memecahkan masalah, dan belajar dari pengalaman (Harahap et al. 2024). Dalam konteks pendidikan, AI digunakan untuk menciptakan lingkungan belajar yang adaptif dan personal melalui teknologi seperti chatbot, recommender system, automatic grading, serta pembelajaran berbasis kecerdasan buatan yang mampu menganalisis kebutuhan dan gaya belajar masing-masing mahasiswa. AI menjadi alat bantu yang berpotensi meningkatkan efisiensi dan kualitas pembelajaran sekaligus mendorong pengalaman belajar yang lebih interaktif dan bermakna.\n",
      "\n",
      "Pembelajaran Digital adalah proses belajar yang didukung oleh penggunaan teknologi digital, baik dalam bentuk Learning Management System (LMS), aplikasi edukatif, video pembelajaran, platform diskusi daring, maupun sistem berbasis AI (Aldahwan & Alsaeed, 2020). Pembelajaran digital memfasilitasi akses informasi yang lebih luas dan fleksibel, serta membuka peluang bagi desain pembelajaran yang kolaboratif, multimodal, dan berbasis data.\n",
      "\n",
      "Di era transformasi pendidikan, pembelajaran digital menjadi solusi utama untuk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for index in range(start, n):\n",
    "    time.sleep(2)\n",
    "    row = \"\"\n",
    "    with open(f\"../data/raw/ocr_result/ocr_{index}.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        row = file.read()\n",
    "\n",
    "    gt = \"\"\n",
    "    with open(f\"../data/raw/ground_truth/gt_{index}.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        gt = file.read()\n",
    "\n",
    "    try:\n",
    "        model = \"gemini-2.0-flash\"\n",
    "        contents = [\n",
    "            types.Content(\n",
    "                role=\"user\",\n",
    "                parts=[\n",
    "                    types.Part.from_text(text=return_prompt(row)),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "        generate_content_config = types.GenerateContentConfig(\n",
    "            response_mime_type=\"text/plain\",\n",
    "        )\n",
    "\n",
    "        response_text=\"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=generate_content_config,\n",
    "        ): response_text += chunk.text\n",
    "\n",
    "        print(index)\n",
    "        print(response_text)\n",
    "        \n",
    "        with open(f\"./LLM_res/res_{index}.txt\", \"w\") as file:\n",
    "            file.write(response_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Row - Error processing column :\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(gt, row, True)\n",
    "# evaluate(gt, response_text, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"eval_baseline.txt\", \"a\") as file:\n",
    "#     file.write(str(baseline))\n",
    "# with open(\"eval_trained.txt\", \"a\") as file:\n",
    "#     file.write(str(trained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65380779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71451822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
