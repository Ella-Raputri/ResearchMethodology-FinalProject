{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c05bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.genai import types\n",
    "from google import genai\n",
    "import re, os, json\n",
    "from jiwer import wer, cer\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import process\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "print(API_KEY)\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb821973",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 424\n",
    "n = 425\n",
    "text_list = [f'../data/raw/ocr_result/ocr_{x}.txt' for x in [140, 266, 339, 423, 424, 425, 428, 445, 454, 480]]\n",
    "print(len(text_list), text_list[0], text_list[-1])\n",
    "\n",
    "gt_list = [f'../data/raw/ground_truth/ocr_{x}.txt' for x in [140, 266, 339, 423, 424, 425, 428, 445, 454, 480]]\n",
    "print(len(gt_list), gt_list[0], gt_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt(row):\n",
    "    prompt = (\n",
    "        \"Teks berikut adalah hasil OCR:\\n\\n\"\n",
    "        f\"{row}\\n\\n\"\n",
    "        \"Instruksi:\\n\"\n",
    "        \"1. Susun ulang teks agar urut dan mudah dibaca.\\n\"\n",
    "        \"2. Perbaiki typo dan kesalahan pemenggalan kata.\\n\"\n",
    "        \"3. Jangan menambah, mengurangi, atau mengubah informasi apa pun.\\n\"\n",
    "        \"4. Jangan menambah komentar, penjelasan, atau catatan.\\n\"\n",
    "        \"5. Output harus berupa teks final saja, tanpa markdown, tanpa format tambahan.\\n\"\n",
    "        \"6. Jangan melakukan asumsi atau mengisi bagian teks yang hilang.\\n\"\n",
    "        \"7. Teks output **harus berisi kata-kata yang sama dengan input**, kecuali kata yang memang diperbaiki karena typo.\\n\"\n",
    "        \"8. Jangan mengubah struktur kalimat secara berlebihan—hanya rapikan urutan dan perbaiki ejaan.\\n\\n\"\n",
    "        \"PERINTAH PENTING:\\n\"\n",
    "        \"• Jangan membuat kalimat baru.\\n\"\n",
    "        \"• Jangan menghilangkan kata.\\n\"\n",
    "        \"• Jangan melakukan halusinasi.\\n\"\n",
    "        \"• Kembalikan hanya teks yang sudah diperbaiki.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2216b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process\n",
    "from scipy.stats import kendalltau\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def split_into_units(text):\n",
    "    text = text.replace(\"\\r\", \"\").strip()\n",
    "    text = re.sub(r\"(\\S+@\\S+)\\.(\\S+)\", r\"\\1<dot>\\2\", text)\n",
    "\n",
    "    protected = [\n",
    "        \"Vol.\", \"No.\", \"hlm.\", \"eISSN.\", \"p-ISSN.\", \"etc.\"\n",
    "    ]\n",
    "    for p in protected:\n",
    "        text = text.replace(p, p.replace(\".\", \"<dot>\"))\n",
    "    \n",
    "    text = text.replace(\"\\n\", \"<nl>\")\n",
    "\n",
    "    parts = re.split(r\"\\.\\s+(?=[A-Z])\", text)\n",
    "    parts = [p.replace(\"<dot>\", \".\") for p in parts]\n",
    "    parts = [p.replace(\"<nl>\", \" \") for p in parts]\n",
    "    units = [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "    return units\n",
    "\n",
    "\n",
    "def kendall_tau_sentence_level(gt_text, pred_text):\n",
    "    gt_units = split_into_units(gt_text)\n",
    "    pred_units = split_into_units(pred_text)\n",
    "    print(gt_units)\n",
    "    print(pred_units)\n",
    "\n",
    "    gt_order = list(range(len(gt_units)))\n",
    "    matched_indices = []\n",
    "    for unit in pred_units:\n",
    "        match = process.extractOne(unit, gt_units)\n",
    "        if match is None:\n",
    "            continue\n",
    "        matched_indices.append(match[2])  \n",
    "\n",
    "    unique_pred_order = []\n",
    "    seen = set()\n",
    "    for idx in matched_indices:\n",
    "        if idx not in seen:\n",
    "            unique_pred_order.append(idx)\n",
    "            seen.add(idx)\n",
    "\n",
    "    min_len = min(len(gt_order), len(unique_pred_order))\n",
    "    gt_order = gt_order[:min_len]\n",
    "    unique_pred_order = unique_pred_order[:min_len]\n",
    "\n",
    "    if min_len < 2:\n",
    "        return 0\n",
    "\n",
    "    tau, p = kendalltau(gt_order, unique_pred_order)\n",
    "    return tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = []\n",
    "trained = []\n",
    "\n",
    "def pmr(gt, pred):\n",
    "    gt_words = gt.split()\n",
    "    pred_words = pred.split()\n",
    "    length = min(len(gt_words), len(pred_words))\n",
    "    matches = sum(1 for i in range(length) if gt_words[i] == pred_words[i])\n",
    "    return matches / length\n",
    "\n",
    "\n",
    "def evaluate(gt, res, isBaseline):\n",
    "    wer_ = wer(gt, res)\n",
    "    cer_ = cer(gt, res)\n",
    "    pmr_ = pmr(gt, res)\n",
    "    tau_ = kendall_tau_sentence_level(gt, res)\n",
    "\n",
    "    if isBaseline:\n",
    "        baseline.append({\"wer\": wer_, \"cer\": cer_, \"pmr\": pmr_, \"tau\": tau_})\n",
    "    else:\n",
    "        trained.append({\"wer\": wer_, \"cer\": cer_, \"pmr\": pmr_, \"tau\": tau_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ec4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for index in [140, 266, 339, 423, 424, 425, 428, 445, 454, 480]:\n",
    "    time.sleep(2)\n",
    "    row = \"\"\n",
    "    with open(f\"../../data/raw/ocr_result/ocr_{index}.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        row = file.read()\n",
    "\n",
    "    gt = \"\"\n",
    "    with open(f\"../../data/raw/ground_truth/gt_{index}.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        gt = file.read()\n",
    "\n",
    "    try:\n",
    "        model = \"gemini-2.0-flash\"\n",
    "        contents = [\n",
    "            types.Content(\n",
    "                role=\"user\",\n",
    "                parts=[\n",
    "                    types.Part.from_text(text=return_prompt(row)),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "        generate_content_config = types.GenerateContentConfig(\n",
    "            response_mime_type=\"text/plain\",\n",
    "        )\n",
    "\n",
    "        response_text=\"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=generate_content_config,\n",
    "        ): response_text += chunk.text\n",
    "\n",
    "        print(index)\n",
    "        print(response_text)\n",
    "        \n",
    "        with open(f\"./LLM_res/res_{index}.txt\", \"w\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "            file.write(response_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Row - Error processing column :\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(gt, row, True)\n",
    "# evaluate(gt, response_text, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"eval_baseline.txt\", \"a\") as file:\n",
    "#     file.write(str(baseline))\n",
    "# with open(\"eval_trained.txt\", \"a\") as file:\n",
    "#     file.write(str(trained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65380779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71451822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
