{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc679cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f79988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\miniconda3\\envs\\yolo-env1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import random \n",
    "from typing import List, Tuple\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82ca2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d24f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\"Dataset sintetis: setiap item adalah list of sentences (shuffled) dan target order.\n",
    "    Digunakan untuk contoh/training cepat.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_examples=1000, min_sents=3, max_sents=8, vocab=None):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "        for _ in range(n_examples):\n",
    "            n = random.randint(min_sents, max_sents)\n",
    "            sents_dict = {i: f\"Ini kalimat ke {i} yang berisi contoh informasi.\" for i in range(n)}\n",
    "            \n",
    "            shuffled_items = list(sents_dict.items())\n",
    "            random.shuffle(shuffled_items)\n",
    "\n",
    "            shuffled_sents = [sent for _, sent in shuffled_items]   # texts\n",
    "            correct_order = [key for key, _ in shuffled_items]\n",
    "            self.examples.append((shuffled_sents, correct_order))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sents, order = self.examples[idx]\n",
    "        return sents, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c73becd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Ini kalimat ke 0 yang berisi contoh informasi.',\n",
       "  'Ini kalimat ke 2 yang berisi contoh informasi.',\n",
       "  'Ini kalimat ke 1 yang berisi contoh informasi.',\n",
       "  'Ini kalimat ke 3 yang berisi contoh informasi.'],\n",
       " [0, 2, 1, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = SyntheticDataset(10)\n",
    "data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6a52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, device, max_length=64):\n",
    "    \"\"\"Batch sentences and compute bert embeddings later.\n",
    "    Returns:\n",
    "      - batch_sentences: list[list[str]] (batch)\n",
    "      - lengths: list[int]\n",
    "      - target_orders: Tensor(batch, max_len) padded with -1 for unused positions\n",
    "    \"\"\"\n",
    "    batch_sents = [item[0] for item in batch]\n",
    "    batch_orders = [item[1] for item in batch]\n",
    "    lengths = [len(x) for x in batch_sents]\n",
    "    max_len = max(lengths)\n",
    "    # pad target orders with -1\n",
    "    padded_orders = torch.full((len(batch), max_len), -1, dtype=torch.long)\n",
    "    for i, ords in enumerate(batch_orders):\n",
    "        padded_orders[i, :len(ords)] = torch.tensor(ords, dtype=torch.long)\n",
    "    return batch_sents, lengths, padded_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6601dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedder:\n",
    "    def __init__(self, model_name='indobenchmark/indobert-base-p1', device='cpu', freeze=True):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        if freeze:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def embed_sentences(self, batch_sentences: List[List[str]], batch_size=16, max_length=64) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input: list of N examples, each example is list of M_i sentences.\n",
    "        Return: embeddings of shape (N, M, D) where M = max M_i (padded with zeros)\n",
    "        \"\"\"\n",
    "        # flatten all sentences in batch -> do batched encoding\n",
    "        flat_sents = []\n",
    "        lengths = [len(x) for x in batch_sentences]\n",
    "        for sents in batch_sentences:\n",
    "            flat_sents.extend(sents)\n",
    "\n",
    "        # tokenize in batches\n",
    "        encodings = self.tokenizer(flat_sents, padding=True, truncation=True,\n",
    "                                   max_length=max_length, return_tensors='pt')\n",
    "        encodings = {k: v.to(self.device) for k, v in encodings.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encodings)\n",
    "            # use CLS token representation (first token)\n",
    "            # outputs.last_hidden_state: (total_sents, seq_len, hidden)\n",
    "            cls_embeds = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        D = cls_embeds.size(-1)\n",
    "        # reshape back to (batch, M, D)\n",
    "        max_M = max(lengths)\n",
    "        batch_size = len(batch_sentences)\n",
    "        embeds = torch.zeros((batch_size, max_M, D), device=self.device)\n",
    "        idx = 0\n",
    "        for i, L in enumerate(lengths):\n",
    "            if L > 0:\n",
    "                embeds[i, :L, :] = cls_embeds[idx: idx + L]\n",
    "            idx += L\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd5a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetDecoder(nn.Module):\n",
    "    \"\"\"Pointer network decoder. At each step points to one of encoder steps.\n",
    "    Based on: Vinyals et al. (2015)\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dec_rnn = nn.LSTMCell(enc_hidden_dim, dec_hidden_dim)\n",
    "        self.W_ref = nn.Linear(enc_hidden_dim, dec_hidden_dim, bias=False)\n",
    "        self.W_q = nn.Linear(dec_hidden_dim, dec_hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, mask=None, lengths=None, max_decode_len=None, teacher_forcing=None):\n",
    "        # encoder_outputs: (B, M, H_e)\n",
    "        B, M, H_e = encoder_outputs.size()\n",
    "        device = encoder_outputs.device\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, M, dtype=torch.bool, device=device)\n",
    "        if lengths is None:\n",
    "            # try infer from mask (count non-masked positions)\n",
    "            lengths = (~mask).sum(dim=1)\n",
    "        # ensure lengths is tensor on correct device\n",
    "        lengths = torch.as_tensor(lengths, dtype=torch.long, device=device)\n",
    "\n",
    "        if max_decode_len is None:\n",
    "            max_decode_len = M\n",
    "\n",
    "        ref = encoder_outputs\n",
    "        ref_proj = self.W_ref(ref)  # (B, M, D)\n",
    "\n",
    "        # initial decoder state: mean of encoder outputs (projected if needed)\n",
    "        h = encoder_outputs.mean(dim=1)  # (B, H_e)\n",
    "        # ensure correct hidden size for LSTMCell (dec_hidden_dim)\n",
    "        # if dec_hidden_dim != H_e, you should map here. (in your setup they match)\n",
    "        c = torch.zeros(B, h.size(1), device=device)\n",
    "\n",
    "        logits_seq = []\n",
    "        pointers = []\n",
    "        avail_mask = ~mask.clone()  # True means available\n",
    "\n",
    "        # we'll use a simple input vector for the LSTMCell each step:\n",
    "        # zeros of size enc_hidden_dim (input_size)\n",
    "        input_t = torch.zeros(B, H_e, device=device)\n",
    "\n",
    "        for t in range(max_decode_len):\n",
    "            # feed previous step's input (or zeros for simplicity)\n",
    "            h, c = self.dec_rnn(input_t, (h, c))  # (B, dec_hidden_dim)\n",
    "\n",
    "            q = self.W_q(h).unsqueeze(1)  # (B,1,D)\n",
    "            scores = self.v(torch.tanh(ref_proj + q)).squeeze(-1)  # (B, M)\n",
    "\n",
    "            # mask out already chosen or padding positions\n",
    "            scores = scores.masked_fill(~avail_mask, float('-inf'))\n",
    "\n",
    "            logits_seq.append(scores)\n",
    "            probs = F.log_softmax(scores, dim=-1)\n",
    "\n",
    "            if teacher_forcing is not None:\n",
    "                # teacher_forcing: (B, T) with -1 for padded decode steps\n",
    "                raw_idx = teacher_forcing[:, t]  # may contain -1\n",
    "                # For safety, clamp to valid range (but don't mark avail_mask for padded sequences)\n",
    "                # valid_step mask: whether this decode step exists for each example\n",
    "                valid_step = (t < lengths).to(device)  # (B,)\n",
    "                # make a safe idx: clamp negative to 0 (or any valid index)\n",
    "                safe_idx = torch.clamp(raw_idx, 0, M-1).to(device)\n",
    "                # But we must only use safe_idx where valid_step True; otherwise ignore updates\n",
    "                idx = safe_idx\n",
    "            else:\n",
    "                idx = probs.exp().multinomial(1).squeeze(-1)  # (B,)\n",
    "\n",
    "            # update avail_mask only for examples where this decode step is valid\n",
    "            valid_step = (t < lengths).to(device)\n",
    "            # build one_hot only for valid examples to avoid out-of-range errors\n",
    "            # create one_hot for all, but mask later when updating avail_mask\n",
    "            # print(\"t\", t, \"idx min/max\", idx.min().item(), idx.max().item(), \"lengths\", lengths)\n",
    "            one_hot = F.one_hot(idx.clamp(0, M-1), num_classes=M).bool()  # (B, M)\n",
    "            # zero-out rows corresponding to invalid steps so avail_mask not changed\n",
    "            one_hot = one_hot & valid_step.unsqueeze(1)\n",
    "\n",
    "            avail_mask = avail_mask & (~one_hot)\n",
    "            pointers.append(idx)\n",
    "\n",
    "        logits = torch.stack(logits_seq, dim=1)  # (B, T, M)\n",
    "        pointers = torch.stack(pointers, dim=1)  # (B, T)\n",
    "        return logits, pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d023348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SONModel(nn.Module):\n",
    "    def __init__(self, enc_input_dim, enc_hidden_dim=512, dec_hidden_dim=512, bidirectional=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bi = 2 if bidirectional else 1\n",
    "        self.enc_lstm = nn.LSTM(enc_input_dim, enc_hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        self.reduce_dim = nn.Linear(enc_hidden_dim * self.bi, enc_hidden_dim)\n",
    "        self.decoder = PointerNetDecoder(enc_hidden_dim, dec_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, sent_embs, lengths, teacher_forcing=None):\n",
    "        # sent_embs: (B, M, D)\n",
    "        lengths = torch.as_tensor(lengths, dtype=torch.long, device=sent_embs.device)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(sent_embs, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.enc_lstm(packed)\n",
    "        enc_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        # enc_out: (B, M, enc_hidden_dim * num_directions)\n",
    "        enc_out = self.reduce_dim(enc_out)  # map to enc_hidden_dim\n",
    "        enc_out = torch.tanh(enc_out)\n",
    "        enc_out = self.dropout(enc_out)\n",
    "\n",
    "        max_len = enc_out.size(1)\n",
    "        seq_range = torch.arange(max_len, device=enc_out.device).unsqueeze(0)\n",
    "        mask = seq_range >= lengths.unsqueeze(1)  # True for padded positions\n",
    "\n",
    "        # pass lengths into decoder so it can know when each example should stop decoding\n",
    "        logits, pointers = self.decoder(enc_out, mask=mask, lengths=lengths, max_decode_len=max_len, teacher_forcing=teacher_forcing)\n",
    "        return logits, pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7ebf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_loss(logits, targets, lengths):\n",
    "    \"\"\"\n",
    "    logits: (B, T, M) logit scores\n",
    "    targets: (B, T) indices (if padded target is -1, ignore)\n",
    "    lengths: list\n",
    "    \"\"\"\n",
    "    B, T, M = logits.size()\n",
    "    loss = 0.0\n",
    "    total = 0\n",
    "    for i in range(B):\n",
    "        L = lengths[i]\n",
    "        if L == 0:\n",
    "            continue\n",
    "        valid_T = L\n",
    "        logp = F.log_softmax(logits[i, :valid_T], dim=-1)\n",
    "        # gather log-probs of correct indices\n",
    "        tgt = targets[i, :valid_T]\n",
    "        loss_i = -logp[range(valid_T), tgt].sum()\n",
    "        loss += loss_i\n",
    "        total += valid_T\n",
    "    return loss / max(1, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28768fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_example(device='cpu'):\n",
    "    # config\n",
    "    MODEL_NAME = 'indobenchmark/indobert-base-p1'  # ganti kalau mau\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # data\n",
    "    dataset = SyntheticDataset(n_examples=500)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True,\n",
    "                            collate_fn=lambda b: collate_fn(b, tokenizer, device))\n",
    "\n",
    "    # embedder\n",
    "    embedder = SentenceEmbedder(model_name=MODEL_NAME, device=device, freeze=True)\n",
    "    # get embedding dim by running a small batch\n",
    "    sample_batch = [dataset[0][0]]\n",
    "    emb = embedder.embed_sentences(sample_batch)\n",
    "    D = emb.size(-1)\n",
    "\n",
    "    # model\n",
    "    model = SONModel(enc_input_dim=D).to(device)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(30):\n",
    "        total_loss = 0.0\n",
    "        for batch_sents, lengths, padded_orders in dataloader:\n",
    "            # embed sentences\n",
    "            sent_embs = embedder.embed_sentences(batch_sents)\n",
    "            sent_embs = sent_embs.to(device)\n",
    "            padded_orders = padded_orders.to(device)\n",
    "            lengths = lengths\n",
    "\n",
    "            # teacher forcing: we feed the *correct* order indices as decoder input\n",
    "            # For pointer net we need for each step the index of the sentence to point to.\n",
    "            # Here teacher_forcing is just the target sequence (original order indices)\n",
    "            teacher = padded_orders.clone()\n",
    "\n",
    "            logits, preds = model.forward(sent_embs, lengths, teacher_forcing=teacher)\n",
    "            loss = pointer_loss(logits, teacher, lengths)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # testing on one example\n",
    "    model.eval()\n",
    "    ex_sents, _ = dataset[1]\n",
    "    print(\"\\nExample shuffled input:\\n\", ex_sents)\n",
    "    with torch.no_grad():\n",
    "        emb = embedder.embed_sentences([ex_sents]).to(device)\n",
    "        logits, preds = model(emb, [len(ex_sents)], teacher_forcing=None)\n",
    "        print(\"Predicted pointer order:\", preds[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17d98aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  8 14:02:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.07                 Driver Version: 566.07         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   55C    P8              9W /   40W |     841MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      9472      C   ...iniconda3\\envs\\yolo-env1\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "413a52c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0469\n",
      "Epoch 2, Loss: 1.0245\n",
      "Epoch 3, Loss: 1.0089\n",
      "Epoch 4, Loss: 1.0002\n",
      "Epoch 5, Loss: 1.0009\n",
      "Epoch 6, Loss: 0.9704\n",
      "Epoch 7, Loss: 0.9257\n",
      "Epoch 8, Loss: 0.8894\n",
      "Epoch 9, Loss: 0.8383\n",
      "Epoch 10, Loss: 0.8095\n",
      "Epoch 11, Loss: 0.8041\n",
      "Epoch 12, Loss: 0.7668\n",
      "Epoch 13, Loss: 0.7209\n",
      "Epoch 14, Loss: 0.6950\n",
      "Epoch 15, Loss: 0.6612\n",
      "Epoch 16, Loss: 0.6228\n",
      "Epoch 17, Loss: 0.5832\n",
      "Epoch 18, Loss: 0.5647\n",
      "Epoch 19, Loss: 0.5468\n",
      "Epoch 20, Loss: 0.5105\n",
      "Epoch 21, Loss: 0.4937\n",
      "Epoch 22, Loss: 0.4793\n",
      "Epoch 23, Loss: 0.4478\n",
      "Epoch 24, Loss: 0.4387\n",
      "Epoch 25, Loss: 0.4196\n",
      "Epoch 26, Loss: 0.4090\n",
      "Epoch 27, Loss: 0.4168\n",
      "Epoch 28, Loss: 0.3852\n",
      "Epoch 29, Loss: 0.3759\n",
      "Epoch 30, Loss: 0.3638\n",
      "\n",
      "Example shuffled input:\n",
      " ['Ini kalimat ke 3 yang berisi contoh informasi.', 'Ini kalimat ke 4 yang berisi contoh informasi.', 'Ini kalimat ke 2 yang berisi contoh informasi.', 'Ini kalimat ke 1 yang berisi contoh informasi.', 'Ini kalimat ke 5 yang berisi contoh informasi.', 'Ini kalimat ke 0 yang berisi contoh informasi.']\n",
      "Predicted pointer order: [4, 3, 2, 1, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # ubah device ke 'cuda' jika tersedia\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    train_example(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d1dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after training loop\n",
    "# torch.save(model.state_dict(), \"son_model.pt\")\n",
    "# print(\"âœ… Model saved as son_model.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
